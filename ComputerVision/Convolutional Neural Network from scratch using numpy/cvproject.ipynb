{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import mnist\nimport numpy as np","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ConvLayer:\n    \"\"\"\n    Convolution Layer with filters having forward and backward pass operations.\n    \"\"\"\n    def __init__(self, num_filters):\n        \"\"\"\n         num_fileters: total number of filters\n         filters: 3d array with dimentions (num_filters, 3, 3)\n        \"\"\"\n        self.num_filters = num_filters\n        self.filters = np.random.randn(num_filters, 3, 3) / 9\n        \n    def get_regions(self, image):\n        \"\"\"\n        # image: matrix of image\n        # getting regions for filters to convolve\n        \"\"\"\n        h, w = image.shape\n        regions_list = []\n        for i in range(h - 2):\n            for j in range(w - 2):\n                im_region = image[i:(i + 3), j:(j + 3)]\n                regions_list.append([im_region,i,j])\n        return regions_list\n        \n                \n    def forward(self, input):\n        \"\"\"\n        Forward pass for convolution layer\n        input: input matrix\n        return: output after convolution operation\n        \"\"\"\n        \n        self.last_input = input\n        h, w = input.shape\n        output = np.zeros((h - 2, w - 2, self.num_filters))\n        regions_list = self.get_regions(input)\n        for im_region, i, j in regions_list:\n            output[i, j] = np.sum(im_region * self.filters, axis=(1, 2))\n            \n        return output\n    \n    def backprop(self, d_out, lr=0.001):\n        \"\"\"\n        d_out: gradient loss\n        lr: learning rate\n        return: None\n        \"\"\"\n        d_filters = np.zeros(self.filters.shape)\n        regions_list = self.get_regions(self.last_input)\n        for im_region, i, j in regions_list:\n            for f in range(self.num_filters):\n                d_filters[f] += d_out[i, j, f] * im_region\n                \n        # Update filters\n        self.filters -= lr * d_filters\n        return None","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Sigmoid:\n    \"\"\"\n    class for calculating forward and backward pass \n    with sigmoid as activation function\n    \"\"\"\n    def __init__(self):\n        self.lastf = None\n        \n    def sigmoid(self,x):\n        return 1/(1+np.exp(-x))\n    \n    def forward(self,x):\n        self.last = self.sigmoid(x)\n        return self.last\n    \n    def backprop(self,x):\n        sig = self.sigmoid(self.last)\n        return x * sig * (1 - sig)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Relu:\n    \"\"\"\n    class for calculating forward and backward pass \n    with Relu as activation function\n    \"\"\"\n    def __init__(self):\n        self.lastf = None\n    \n    def forward(self,x):\n        self.lastf = x\n        return np.maximum(0,x)\n    \n    def backprop(self,x):\n        dZ = np.array(self.lastf, copy = True)\n        dZ[x <= 0] = 0;\n        return dZ;","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MaxPool:\n    \"\"\"\n    Maxpool: decrease size of input matrix by half. Picks up maximum val in window.\n    \"\"\"\n    def __init__(self):\n        pass\n    \n    def get_regions(self, image):\n        '''\n        Generates image regions image regions to pool over.\n        '''\n        h, w, _ = image.shape\n        new_h = h // 2\n        new_w = w // 2\n        \n        regions_list = []\n        \n        for i in range(new_h):\n            for j in range(new_w):\n                im_region = image[(i * 2):(i * 2 + 2), (j * 2):(j * 2 + 2)]\n                regions_list.append([im_region,i,j])\n                \n        return regions_list\n        \n\n    def forward(self, input):\n        '''\n        Performs a forward pass of the maxpool layer using the given input.\n        Returns a 3d numpy array with dimensions (h / 2, w / 2, num_filters).\n        input is a 3d numpy array with dimensions (h, w, num_filters)\n        '''\n        \n        self.last_input = input\n        h, w, num_filters = input.shape\n        output = np.zeros((h // 2, w // 2, num_filters))\n        \n        regions_list = self.get_regions(input)\n        for im_region, i, j in regions_list:\n            output[i, j] = np.amax(im_region, axis=(0, 1))\n        \n        return output\n    \n    def backprop(self, d_out):\n        \"\"\"\n        backprop\n        d_out: incoming gradient loss\n        return: outgoing gradient loss\n        \"\"\"\n        \n        d_input = np.zeros(self.last_input.shape)\n        regions_list = self.get_regions(self.last_input)\n        for im_region, i, j in regions_list:\n            h, w, f = im_region.shape\n            amax = np.amax(im_region, axis=(0, 1))\n            \n            for i2 in range(h):\n                for j2 in range(w):\n                    for f2 in range(f):\n                        if im_region[i2, j2, f2] == amax[f2]:\n                            d_input[i + i2, j + j2, f2] = d_out[i, j, f2]\n                            \n        return d_input","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Dense:\n    def __init__(self,n,input_len):\n        \"\"\"\n        n: number of neurons\n        input_len: length of input\n        \"\"\"\n        self.neurons = n\n        self.weights = np.random.randn(input_len, n) / input_len\n        self.biases = np.zeros(n)\n        self.last = None\n        self.total = None\n        self.lastinput = None\n        self.lastinputshape = None\n        \n    def forward(self,input):\n        \"\"\"\n        input: matrix\n        o/p: sigmoid(wx+b)\n        \"\"\"\n        self.lastinputshape = input.shape\n        input = input.flatten()\n        self.lastinput = input\n        z1 = np.dot(self.weights.T,input) + self.biases\n        \n        self.total = z1\n        a1 = 1/(1+np.exp(-z1))\n        self.last = a1\n        return a1\n\n    def sigmoid(self,x):\n        return 1/(1+np.exp(-x))\n\n    def backprop(self,gradient,lr):\n        \"\"\"\n        gradient: gradient loss from following layers\n        lr: learning rate\n        o/p: loss to be propagated\n        \"\"\"\n        z1 = self.total\n        derivative = self.sigmoid(z1)*(1-self.sigmoid(z1))\n        dtd= derivative*gradient\n        # print(derivative.shape)\n        # print(self.lastinput.shape)\n        \n        dldw = np.matmul(self.lastinput[np.newaxis].T,dtd[np.newaxis])\n        # print(dldw.shape)\n        # print(dldw)\n        \n        dldb = dtd\n        dlinput = self.weights\n        \n        dldinp=np.matmul(self.weights,dtd)\n        \n        # updating weights and biases\n        # w = w - lr*d/dw\n        # b = b - lr*d/db\n        self.weights -= lr*dldw\n        self.biases -= lr*dldb\n        \n        return dldinp.reshape(self.lastinputshape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Softmax:\n    \"\"\"\n    Dense Layer with Softmax activation function\n    \"\"\"\n    \n    def __init__(self, input_len,n):\n        \"\"\"\n        n: number of neurons\n        input_len: input size\n        \"\"\"\n        self.weights = np.random.randn(input_len, n) / input_len\n        self.biases = np.zeros(n)\n\n    def forward(self, input):\n        '''\n        Performs a forward pass of the softmax layer using the given input.\n        Returns a 1d numpy array containing the respective probability values.\n        - input can be any array with any dimensions.\n        '''\n        self.last_input_shape = input.shape\n        self.last_input = input\n        input_len, nodes = self.weights.shape\n        z2 = np.dot(input, self.weights) + self.biases\n        self.last_totals = z2\n        \n        # activation function : softmax\n        exp = np.exp(z2)\n        return exp / np.sum(exp, axis=0)\n    \n    def backprop(self, d_out, lr):\n        \"\"\"\n        d_out: loss from following layers\n        lr: learning rate\n        o/p: loss to be propagated\n        \"\"\"\n        for i, gradient in enumerate(d_out):\n            if gradient == 0:\n                continue\n            t_exp = np.exp(self.last_totals)\n        \n            # Sum of all e^totals\n            S = np.sum(t_exp)\n            \n            # changing only set value\n            d_outdz = -t_exp[i] * t_exp / (S ** 2)\n            # change the value of k == c\n            d_outdz[i] = t_exp[i] * (S - t_exp[i]) / (S ** 2)\n        \n            #derivative \n            d_outdw = self.last_input \n            d_outdb = 1\n            d_outdinp = self.weights\n        \n            # Gradients of loss against totals\n            dg = gradient * d_outdz\n        \n            d_L_d_w = np.matmul(d_outdw[np.newaxis].T , dg[np.newaxis])\n            d_L_d_b = dg * d_outdb\n            d_L_d_inputs = np.matmul( d_outdinp , dg)\n        \n            # Update weights / biases\n            self.weights -= (lr * d_L_d_w)\n            self.biases -= (lr * d_L_d_b)\n        \n            # it will be used in previous pooling layer\n            # reshape into that matrix\n            return d_L_d_inputs.reshape(self.last_input_shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We only use the first 1k examples of each set in the interest of time.\n# Feel free to change this if you want.\ntrain_images = mnist.train_images()[:1000]\ntrain_labels = mnist.train_labels()[:1000]\ntest_images = mnist.test_images()[:1000]\ntest_labels = mnist.test_labels()[:1000]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"conv = ConvLayer(8)                    # 28x28x1 -> 26x26x8\npool = MaxPool()                    # 26x26x8 -> 13x13x8\ndense = Dense(20,13 * 13 * 8)\nsoftmax = Softmax(20, 10)    # 13x13x8 -> 10\nact = Sigmoid()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def forward(image, label):\n    '''\n    Completes a forward pass of the CNN and calculates the accuracy and\n    cross-entropy loss.\n    - image is a 2d numpy array\n    - label is a digit\n    '''\n    out = conv.forward((image / 255))\n    out = act.forward(out)\n    out = pool.forward(out)\n    out = dense.forward(out)\n    out = softmax.forward(out)\n    \n    # Calculate cross-entropy loss \n    loss = -np.log(out[label])\n    acc = 1 if np.argmax(out) == label else 0\n\n    return out, loss, acc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def backward(gradient,lr):\n    \"\"\"\n    Backward Controller for entire model\n    gradient: loss\n    lr: learning rate\n    \"\"\"\n    gradient = softmax.backprop(gradient, lr)\n    gradient = dense.backprop(gradient,lr)\n    gradient = pool.backprop(gradient)\n    gradient = act.backprop(gradient) \n    gradient = conv.backprop(gradient, lr)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef train(im, label, lr=0.01):\n    \"\"\"\n    Controller for forward , backward and loss of entire model\n    im: image matrix\n    label: label/class of image\n    lr: learning rate\n    \"\"\"\n    out, loss, acc = forward(im, label)\n    gradient = np.zeros(10)\n    gradient[label] = -1 / out[label]\n    backward(gradient,lr)\n    return loss, acc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for epoch in range(100):\n    print('--- Epoch %d ---' % (epoch + 1))    \n    \n    loss = 0\n    num_correct = 0\n    for i, (im, label) in enumerate(zip(train_images, train_labels)):\n        l, acc = train(im, label)\n        loss += l\n        num_correct += acc\n    print(loss/1000,num_correct/1000)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test the CNN\nprint('\\n--- Testing the CNN ---')\nloss = 0\n\nnum_correct = 0\nfor im, label in zip(test_images, test_labels):\n    _, l, acc = forward(im, label)\n    loss += l\n    num_correct += acc\n\nnum_tests = len(test_images)\nprint('Test Loss:', loss / num_tests)\nprint('Test Accuracy:', num_correct / num_tests)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc \ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##### TESTING ############\n# img = None\n# for i,j in enumerate(train_images):\n#     img = j\n#     if i==1:\n#         break\n# test = forward(img,0)\n# # ss = np.array([1.0]*1352)\n# d = Dense(30,13*13*8)\n# dd = d.forward(test)\n# dd.shape\n# ss = np.array([1.0]*30)\n# out = d.backprop(ss,0.1)\n# print(out)\n# print(out.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}